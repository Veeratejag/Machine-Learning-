{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.csv\")\n",
    "# ax, fig = plt.subplots()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"year,month,toss,day_match,bat_first,format,fow,score,rpo,result\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = data['year']\n",
    "tosses = data['toss']\n",
    "result = data['result']\n",
    "day_match = data['day_match']\n",
    "bat_first = data['bat_first']\n",
    "format = data['format']\n",
    "fow = data['fow']\n",
    "score = data['score']\n",
    "rpo = data['rpo']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter,deque\n",
    "Counter([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "def get_np_array(file_name):\n",
    "    label_encoder = None \n",
    "    data = pd.read_csv(file_name)\n",
    "    \n",
    "    need_label_encoding = ['team','host','opp','month', 'day_match']\n",
    "    if(label_encoder is None):\n",
    "        label_encoder = OneHotEncoder(sparse_output = False)\n",
    "        label_encoder.fit(data[need_label_encoding])\n",
    "    data_1 = pd.DataFrame(label_encoder.transform(data[need_label_encoding]), columns = label_encoder.get_feature_names_out())\n",
    "    # print(data_1.shape)\n",
    "    #merge the two dataframes\n",
    "    dont_need_label_encoding =  [\"year\",\"toss\",\"bat_first\",\"format\" ,\"fow\",\"score\" ,\"rpo\" ,\"result\"]\n",
    "    data_2 = data[dont_need_label_encoding]\n",
    "    final_data = pd.concat([data_1, data_2], axis=1)\n",
    "    \n",
    "    X = final_data.iloc[:,:-1]\n",
    "    y = final_data.iloc[:,-1:]\n",
    "    return X.to_numpy(), y.to_numpy()\n",
    "\n",
    "X_train,y_train = get_np_array('train.csv')\n",
    "X_test, y_test = get_np_array(\"test.csv\")\n",
    "\n",
    "#only needed in part (c)\n",
    "X_val, y_val = get_np_array(\"val.csv\")\n",
    "\n",
    "types = ['cat','cat','cat',\"cat\",\"cat\",\"cont\",\"cat\",\"cat\",\"cat\" ,\"cont\",\"cont\" ,\"cont\" ]\n",
    "while(len(types) != X_train.shape[1]):\n",
    "    types = ['cat'] + types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DTNode:\n",
    "\n",
    "    def __init__(self, depth, is_leaf = False, value = 0,threshold=None, column = None):\n",
    "\n",
    "        #to split on column\n",
    "        self.depth = depth\n",
    "        self.threshold = threshold\n",
    "        #add children afterwards\n",
    "        self.children = None\n",
    "\n",
    "        #if leaf then also need value\n",
    "        self.is_leaf = is_leaf\n",
    "        if(self.is_leaf):\n",
    "            self.value = value\n",
    "        \n",
    "        if(not self.is_leaf):\n",
    "            self.column = column\n",
    "\n",
    "\n",
    "    def get_children(self, X):\n",
    "        '''\n",
    "        Args:\n",
    "            X: A single example np array [num_features]\n",
    "        Returns:\n",
    "            child: A DTNode\n",
    "        '''\n",
    "        if self.is_leaf:\n",
    "            return self\n",
    "        elif types[self.column]==\"cat\":\n",
    "            # print(self.column, X[self.column],len(self.children))\n",
    "            if X[self.column] >= len(self.children):\n",
    "                return self.children[-1]\n",
    "            return self.children[int(X[self.column])]\n",
    "        else:\n",
    "            if X[self.column] <= self.threshold:\n",
    "                return self.children[0]\n",
    "            else:\n",
    "                return self.children[1]\n",
    "        \n",
    "def entropy(X,y):\n",
    "    if(len(y)==0):\n",
    "        return 0\n",
    "    entropy=0\n",
    "    for i in np.unique(y):\n",
    "        p=np.sum(y==i)/len(y)\n",
    "        if p==1 or p==0:\n",
    "            return 0\n",
    "        entropy-=p*np.log2(p)\n",
    "    return entropy\n",
    "\n",
    "def information_gain(X,y,feature,iscat:bool):\n",
    "    entropy_parent=entropy(X,y)\n",
    "    final_entropy=0\n",
    "    if not iscat:\n",
    "        threshold=np.median(X[:,feature])\n",
    "        left_X=X[X[:,feature]<=threshold]\n",
    "        right_X=X[X[:,feature]>threshold]\n",
    "        left_y=y[X[:,feature]<=threshold]\n",
    "        right_y=y[X[:,feature]>threshold]\n",
    "        p=len(left_y)/len(y)\n",
    "        final_entropy=p*entropy(left_X,left_y)+(1-p)*entropy(right_X,right_y)\n",
    "    else:\n",
    "        children = []\n",
    "        values=np.unique(X[:,feature])\n",
    "        for i in values:\n",
    "            child_X = X[X[:, feature] == i]\n",
    "            child_y = y[X[:, feature] == i]\n",
    "            children.append((child_X, child_y))\n",
    "        \n",
    "        for child_X, child_y in children:\n",
    "            final_entropy += len(child_y) / len(y) * entropy(child_X, child_y)\n",
    "    return entropy_parent-final_entropy\n",
    "def split(X,y,types):\n",
    "    best_feature=None\n",
    "    best_ig=-1\n",
    "    best_threshold=None\n",
    "    for i in range(len(types)):\n",
    "        ig=information_gain(X,y,i,types[i]=='cat')\n",
    "        if(ig>best_ig):\n",
    "            best_ig=ig\n",
    "            best_feature=i\n",
    "            if(types[i]=='cat'):\n",
    "                best_threshold=None\n",
    "            else:\n",
    "                best_threshold=np.median(X[:,i])\n",
    "    return best_feature,best_threshold\n",
    "\n",
    "class DTTree:\n",
    "\n",
    "    def __init__(self):\n",
    "        #Tree root should be DTNode\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, y, types, max_depth = 10):\n",
    "        '''\n",
    "        Makes decision tree\n",
    "        Args:\n",
    "            X: numpy array of data [num_samples, num_features]\n",
    "            y: numpy array of classes [num_samples, 1]\n",
    "            types: list of [num_features] with types as: cat, cont\n",
    "                eg: if num_features = 4, and last 2 features are continious then\n",
    "                    types = ['cat','cat','cont','cont']\n",
    "            max_depth: maximum depth of tree\n",
    "        Returns:\n",
    "            None\n",
    "        '''\n",
    "        self.root = self.grow_tree(X, y, types, max_depth, 0)\n",
    "        #TODO\n",
    "    def grow_tree(self, X, y, types, max_depth, depth):\n",
    "        if depth == max_depth or len(np.unique(y)) == 1:\n",
    "            return DTNode(depth, is_leaf = True, value = np.bincount(y.flatten()).argmax())\n",
    "        else:\n",
    "            best_col, best_split =split(X, y, types)\n",
    "            if best_col is None:\n",
    "                print(\"best col is none\", depth)\n",
    "                return DTNode(depth, is_leaf = True, value = np.bincount(y.flatten()).argmax())\n",
    "            if np.unique(X[:,best_col]).shape[0] ==1:\n",
    "                return DTNode(depth, is_leaf = True, value = np.bincount(y.flatten()).argmax())\n",
    "            else:\n",
    "                node = DTNode(depth, is_leaf = False,threshold=best_split, column = best_col)\n",
    "                if types[best_col] == \"cat\":\n",
    "                    node.children = []\n",
    "                    for i in np.unique(X[:,best_col]):\n",
    "                        child_X = X[X[:,best_col]==i]\n",
    "                        child_y = y[X[:,best_col]==i]\n",
    "                        child = self.grow_tree(child_X, child_y, types, max_depth, depth+1)\n",
    "                        node.children.append(child)\n",
    "                    \n",
    "                else:\n",
    "                    left_X = X[X[:,best_col]<=best_split]\n",
    "                    left_y = y[X[:,best_col]<=best_split]\n",
    "                    right_X = X[X[:,best_col]>best_split]\n",
    "                    right_y = y[X[:,best_col]>best_split]\n",
    "                    left_subtree = self.grow_tree(left_X, left_y, types, max_depth, depth+1)\n",
    "                    right_subtree = self.grow_tree(right_X, right_y, types, max_depth, depth+1)\n",
    "                    node.children = [left_subtree, right_subtree]\n",
    "                return node\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        '''\n",
    "        Predicted classes for X\n",
    "        Args:\n",
    "            X: numpy array of data [num_samples, num_features]\n",
    "        Returns:\n",
    "            y: [num_samples, 1] predicted classes\n",
    "        '''\n",
    "        y_pred = []\n",
    "        for x in X:\n",
    "            y_pred.append(self.predict(x))\n",
    "        return y_pred\n",
    "    def predict(self, x):\n",
    "        node = self.root\n",
    "        while node.is_leaf == False:\n",
    "            node = node.get_children(x)\n",
    "        return node.value\n",
    "    def post_prune(self, X_val, y_val):\n",
    "        node = self.root\n",
    "        if not node:\n",
    "            return node\n",
    "        bfs = deque([node])\n",
    "        while bfs:\n",
    "            curr = bfs.popleft()\n",
    "            tree_sub = DTTree()\n",
    "            tree_sub.root = curr\n",
    "            y_pred = tree_sub(X_val)\n",
    "            acc = np.sum(y_pred == y_val.flatten())/len(y_val)\n",
    "            y_p = np.argmax(np.bincount(y_val.flatten()))\n",
    "            if acc<np.mean(y_p==y_val):\n",
    "                curr.is_leaf = True\n",
    "                curr.value = np.bincount(y_val.flatten()).argmax()\n",
    "            else:\n",
    "                if curr.children:\n",
    "                    bfs.extend(curr.children)\n",
    "        self.root = node\n",
    "        # pass\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "# from DecisionTree import DecisionTree\n",
    "\n",
    "X,y = get_np_array(\"train.csv\")\n",
    "X_test,y_test = get_np_array(\"test.csv\")\n",
    "X_val,y_val = get_np_array(\"val.csv\")\n",
    "clf = DecisionTree(max_depth=5)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "def accuracy(y_test, y_pred):\n",
    "    return np.sum(y_test == y_pred) / len(y_test)\n",
    "\n",
    "y_pred = clf.predict(X_train)\n",
    "acc = accuracy(y_train, y_pred)\n",
    "print (\"Train Accuracy:\", acc)\n",
    "# y_pred = clf.predict(X_test)\n",
    "# acc = accuracy(y_test, y_pred)\n",
    "# print (\"Test Accuracy:\", acc)\n",
    "y_pred = clf.predict(X_val)\n",
    "acc = accuracy(y_val, y_pred)\n",
    "print (\"Val Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for max_depth in [15, 25, 35, 45]:\n",
    "    print(\"Max Depth: \",max_depth)\n",
    "    tree = DTTree()\n",
    "    tree.fit(X_train,y_train,types, max_depth = max_depth)\n",
    "    # tree.post_prune(X_val, y_val)\n",
    "    y_pred = tree(X_train)\n",
    "    print(\"Training Accuracy: \",np.mean(y_train==y_pred))\n",
    "    y_pred = tree(X_test)\n",
    "    print(\"Testing Accuracy: \",np.mean(y_test==y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = get_np_array2(\"train.csv\")\n",
    "X_test,y_test = get_np_array2(\"test.csv\")\n",
    "X_val,y_val = get_np_array2(\"val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=0\n",
    "def accuracy(y_test, y_pred):\n",
    "    return np.sum(y_test == y_pred) / len(y_test)\n",
    "print(\"Train Accuracy:\",accuracy(y_val,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "d={}\n",
    "for depth in {15, 25, 35, 45}:\n",
    "    tree = DecisionTreeClassifier(max_depth=depth, criterion=\"entropy\")\n",
    "    tree.fit(X, y)\n",
    "    print(\"Depth:\", depth)\n",
    "    \n",
    "    # Calculate accuracy for the training set\n",
    "    y_train_pred = tree.predict(X)\n",
    "    train_accuracy = accuracy_score(y, y_train_pred)\n",
    "    print(\"Train Accuracy:\", train_accuracy)\n",
    "\n",
    "    # Calculate accuracy for the test set\n",
    "    y_test_pred = tree.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "    # Calculate accuracy for the validation set (if defined)\n",
    "    if 'X_val' in locals() and 'y_val' in locals():\n",
    "        y_val_pred = tree.predict(X_val)\n",
    "        val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "        print(\"Val Accuracy:\", val_accuracy)\n",
    "        d[depth]=val_accuracy\n",
    "    print()\n",
    "final_depth=max(d,key=d.get)\n",
    "print(\"Final Depth:\",final_depth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "d={}\n",
    "for ccp in {0.001, 0.01, 0.1, 0.2}:\n",
    "    tree = DecisionTreeClassifier( criterion=\"entropy\",ccp_alpha=ccp)\n",
    "    tree.fit(X, y)\n",
    "    print(\"CCP:\", ccp)\n",
    "    \n",
    "    # Calculate accuracy for the training set\n",
    "    y_train_pred = tree.predict(X)\n",
    "    train_accuracy = accuracy_score(y, y_train_pred)\n",
    "    print(\"Train Accuracy:\", train_accuracy)\n",
    "\n",
    "    # Calculate accuracy for the test set\n",
    "    y_test_pred = tree.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "    # Calculate accuracy for the validation set (if defined)\n",
    "    if 'X_val' in locals() and 'y_val' in locals():\n",
    "        y_val_pred = tree.predict(X_val)\n",
    "        val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "        print(\"Val Accuracy:\", val_accuracy)\n",
    "        d[ccp]=val_accuracy\n",
    "    print()\n",
    "final_ccp=max(d,key=d.get)\n",
    "print(\"Final CCP:\",final_ccp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "rf_model = RandomForestClassifier(oob_score=True, random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': range(50, 351, 100),\n",
    "    'max_features': [0.1, 0.3, 0.5, 0.7, 0.9, 1.0],\n",
    "    'min_samples_split': range(2, 11, 2)\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X, y)\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "y_train_pred = best_model.predict(X)\n",
    "train_accuracy = accuracy_score(y, y_train_pred)\n",
    "if X_val is not None and y_val is not None:\n",
    "    y_val_pred = best_model.predict(X_val)\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "else:\n",
    "    val_accuracy = None\n",
    "\n",
    "oob_accuracy = best_model.oob_score_\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Training set accuracy:\", train_accuracy)\n",
    "if val_accuracy is not None:\n",
    "    print(\"Validation set accuracy:\", val_accuracy)\n",
    "print(\"Out-of-bag accuracy:\", oob_accuracy)\n",
    "print(\"Test set accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X_train,y_train = get_np_array2('train.csv')\n",
    "X_test, y_test = get_np_array2(\"test.csv\")\n",
    "X_val, y_val = get_np_array2(\"val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model=[]\n",
    "best_Acc=0\n",
    "for depth in {5, 10, 15, 20}:\n",
    "    for min_samples in [2, 4, 6, 8]:\n",
    "        for max_featue in [0.1, 0.3, 0.5, 0.7, 0.9, 1.0]:\n",
    "            print(\"Depth:\",depth,\"min_samples:\",min_samples,\"max_feature:\",max_featue)\n",
    "            clf = GradientBoostingClassifier(n_estimators=350, learning_rate=1.0,\n",
    "                max_depth=depth,min_samples_split=min_samples ,max_features=max_featue,random_state=56).fit(X_train, y_train)\n",
    "            # print(clf.score(X_train, y_train))\n",
    "            # print(clf.score(X_test, y_test))\n",
    "            val_acc=clf.score(X_val, y_val)\n",
    "            print(val_acc)\n",
    "            if(val_acc>best_Acc):\n",
    "                best_Acc=val_acc\n",
    "                best_model=[depth,min_samples,max_featue]\n",
    "            print()\n",
    "print(\"Best model:\\n\",\"Depth: \",best_model[0],\"\\nmin_samples: \",best_model[1],\"\\nmax_feature: \",best_model[2],\"\\nAccuracy: \",best_Acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(X,y):\n",
    "    if(len(y)==0):\n",
    "        return 0\n",
    "    entropy=0\n",
    "    for i in np.unique(y):\n",
    "        p=np.sum(y==i)/len(y)\n",
    "        if p==1 or p==0:\n",
    "            return 0\n",
    "        entropy-=p*np.log2(p)\n",
    "    return entropy\n",
    "\n",
    "def information_gain(X,y,feature,iscat:bool):\n",
    "    entropy_parent=entropy(X,y)\n",
    "    if not iscat:\n",
    "        threshold=np.median(X[:,feature])\n",
    "        left_X=X[X[:,feature]<=threshold]\n",
    "        right_X=X[X[:,feature]>threshold]\n",
    "        left_y=y[X[:,feature]<=threshold]\n",
    "        right_y=y[X[:,feature]>threshold]\n",
    "        p=len(left_y)/len(y)\n",
    "        return  p*entropy(left_X,left_y)+(1-p)*entropy(right_X,right_y)\n",
    "    else:\n",
    "        children = []\n",
    "        values=np.unique(X[:,feature])\n",
    "        for i in values:\n",
    "            child_X = X[X[:, feature] == i]\n",
    "            child_y = y[X[:, feature] == i]\n",
    "            children.append((child_X, child_y))\n",
    "        final_entropy = 0\n",
    "        for child_X, child_y in children:\n",
    "            final_entropy += len(child_y) / len(y) * entropy(child_X, child_y)\n",
    "        return final_entropy\n",
    "\n",
    "def split(X,y,types):\n",
    "    best_feature=None\n",
    "    best_ig=1\n",
    "    best_threshold=None\n",
    "    for i in range(len(types)):\n",
    "        ig=information_gain(X,y,i,types[i]=='cat')\n",
    "        if(ig<best_ig):\n",
    "            best_ig=ig\n",
    "            best_feature=i\n",
    "            if(types[i]=='cat'):\n",
    "                best_threshold=None\n",
    "            else:\n",
    "                best_threshold=np.median(X[:,i])\n",
    "    return best_feature,best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def get_np_array(file_name):\n",
    "    label_encoder = None \n",
    "    data = pd.read_csv(file_name)\n",
    "    \n",
    "    need_label_encoding = ['team','host','opp','month', 'day_match']\n",
    "    if(label_encoder is None):\n",
    "        label_encoder = OrdinalEncoder()\n",
    "        label_encoder.fit(data[need_label_encoding])\n",
    "    data_1 = pd.DataFrame(label_encoder.transform(data[need_label_encoding]), columns = label_encoder.get_feature_names_out())\n",
    "    # print(data_1.shape)\n",
    "    #merge the two dataframes\n",
    "    dont_need_label_encoding =  [\"year\",\"toss\",\"bat_first\",\"format\" ,\"fow\",\"score\" ,\"rpo\" ,\"result\"]\n",
    "    data_2 = data[dont_need_label_encoding]\n",
    "    final_data = pd.concat([data_1, data_2], axis=1)\n",
    "    \n",
    "    X = final_data.iloc[:,:-1]\n",
    "    y = final_data.iloc[:,-1:]\n",
    "    return X.to_numpy(), y.to_numpy()\n",
    "\n",
    "def get_np_array2(file_name):\n",
    "    label_encoder = None \n",
    "    data = pd.read_csv(file_name)\n",
    "    \n",
    "    need_label_encoding = ['team','host','opp','month', 'day_match']\n",
    "    if(label_encoder is None):\n",
    "        label_encoder = OneHotEncoder(sparse_output = False)\n",
    "        label_encoder.fit(data[need_label_encoding])\n",
    "    data_1 = pd.DataFrame(label_encoder.transform(data[need_label_encoding]), columns = label_encoder.get_feature_names_out())\n",
    "    # print(data_1.shape)\n",
    "    #merge the two dataframes\n",
    "    dont_need_label_encoding =  [\"year\",\"toss\",\"bat_first\",\"format\" ,\"fow\",\"score\" ,\"rpo\" ,\"result\"]\n",
    "    data_2 = data[dont_need_label_encoding]\n",
    "    final_data = pd.concat([data_1, data_2], axis=1)\n",
    "    \n",
    "    X = final_data.iloc[:,:-1]\n",
    "    y = final_data.iloc[:,-1:]\n",
    "    return X.to_numpy(), y.to_numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = ['cat','cat','cat',\"cat\",\"cat\",\"cont\",\"cat\",\"cat\",\"cat\" ,\"cont\",\"cont\" ,\"cont\" ]\n",
    "X,y = get_np_array(\"train.csv\")\n",
    "# X = X[:,:len(types)]\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(y==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label_encoder = None \n",
    "X_train,y_train = get_np_array('train.csv')\n",
    "label_encoder = None \n",
    "# X_train = X_train[:,:len(types)]\n",
    "X_test, y_test = get_np_array(\"test.csv\")\n",
    "print(np.mean(y_train==0))\n",
    "print(np.mean(y_test==0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class DTNode:\n",
    "\n",
    "    def __init__(self, depth, is_leaf = False, value = 0,threshold=None, column = None):\n",
    "\n",
    "        #to split on column\n",
    "        self.depth = depth\n",
    "        self.threshold = threshold\n",
    "        #add children afterwards\n",
    "        self.children = None\n",
    "\n",
    "        #if leaf then also need value\n",
    "        self.is_leaf = is_leaf\n",
    "        if(self.is_leaf):\n",
    "            self.value = value\n",
    "        \n",
    "        if(not self.is_leaf):\n",
    "            self.column = column\n",
    "\n",
    "\n",
    "    def get_children(self, X):\n",
    "        '''\n",
    "        Args:\n",
    "            X: A single example np array [num_features]\n",
    "        Returns:\n",
    "            child: A DTNode\n",
    "        '''\n",
    "        if self.is_leaf:\n",
    "            return self\n",
    "        elif types[self.column]==\"cat\":\n",
    "            # print(self.column, X[self.column],len(self.children))\n",
    "            if X[self.column] >= len(self.children):\n",
    "                return self.children[-1]\n",
    "            return self.children[int(X[self.column])]\n",
    "        else:\n",
    "            if X[self.column] <= self.threshold:\n",
    "                return self.children[0]\n",
    "            else:\n",
    "                return self.children[1]\n",
    "        \n",
    "def entropy(X,y):\n",
    "    if(len(y)==0):\n",
    "        return 0\n",
    "    entropy=0\n",
    "    for i in np.unique(y):\n",
    "        p=np.sum(y==i)/len(y)\n",
    "        if p==1 or p==0:\n",
    "            return 0\n",
    "        entropy-=p*np.log2(p)\n",
    "    return entropy\n",
    "\n",
    "def information_gain(X,y,feature,iscat:bool):\n",
    "    entropy_parent=entropy(X,y)\n",
    "    final_entropy=0\n",
    "    if not iscat:\n",
    "        threshold=np.median(X[:,feature])\n",
    "        left_X=X[X[:,feature]<=threshold]\n",
    "        right_X=X[X[:,feature]>threshold]\n",
    "        left_y=y[X[:,feature]<=threshold]\n",
    "        right_y=y[X[:,feature]>threshold]\n",
    "        p=len(left_y)/len(y)\n",
    "        final_entropy=p*entropy(left_X,left_y)+(1-p)*entropy(right_X,right_y)\n",
    "    else:\n",
    "        children = []\n",
    "        values=np.unique(X[:,feature])\n",
    "        for i in values:\n",
    "            child_X = X[X[:, feature] == i]\n",
    "            child_y = y[X[:, feature] == i]\n",
    "            children.append((child_X, child_y))\n",
    "        \n",
    "        for child_X, child_y in children:\n",
    "            final_entropy += len(child_y) / len(y) * entropy(child_X, child_y)\n",
    "    return entropy_parent-final_entropy\n",
    "def split(X,y,types):\n",
    "    best_feature=None\n",
    "    best_ig=-1\n",
    "    best_threshold=None\n",
    "    for i in range(len(types)):\n",
    "        ig=information_gain(X,y,i,types[i]=='cat')\n",
    "        if(ig>best_ig):\n",
    "            best_ig=ig\n",
    "            best_feature=i\n",
    "            if(types[i]=='cat'):\n",
    "                best_threshold=None\n",
    "            else:\n",
    "                best_threshold=np.median(X[:,i])\n",
    "    return best_feature,best_threshold\n",
    "\n",
    "class DTTree:\n",
    "\n",
    "    def __init__(self):\n",
    "        #Tree root should be DTNode\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, y, types, max_depth = 10):\n",
    "        '''\n",
    "        Makes decision tree\n",
    "        Args:\n",
    "            X: numpy array of data [num_samples, num_features]\n",
    "            y: numpy array of classes [num_samples, 1]\n",
    "            types: list of [num_features] with types as: cat, cont\n",
    "                eg: if num_features = 4, and last 2 features are continious then\n",
    "                    types = ['cat','cat','cont','cont']\n",
    "            max_depth: maximum depth of tree\n",
    "        Returns:\n",
    "            None\n",
    "        '''\n",
    "        self.root = self.grow_tree(X, y, types, max_depth, 0)\n",
    "        #TODO\n",
    "    def grow_tree(self, X, y, types, max_depth, depth):\n",
    "        if depth == max_depth or len(np.unique(y)) == 1:\n",
    "            return DTNode(depth, is_leaf = True, value = np.bincount(y.flatten()).argmax())\n",
    "        else:\n",
    "            best_col, best_split =split(X, y, types)\n",
    "            if best_col is None:\n",
    "                print(\"best col is none\", depth)\n",
    "                return DTNode(depth, is_leaf = True, value = np.bincount(y.flatten()).argmax())\n",
    "            if np.unique(X[:,best_col]).shape[0] ==1:\n",
    "                return DTNode(depth, is_leaf = True, value = np.bincount(y.flatten()).argmax())\n",
    "            else:\n",
    "                node = DTNode(depth, is_leaf = False,threshold=best_split, column = best_col)\n",
    "                if types[best_col] == \"cat\":\n",
    "                    node.children = []\n",
    "                    for i in np.unique(X[:,best_col]):\n",
    "                        child_X = X[X[:,best_col]==i]\n",
    "                        child_y = y[X[:,best_col]==i]\n",
    "                        child = self.grow_tree(child_X, child_y, types, max_depth, depth+1)\n",
    "                        node.children.append(child)\n",
    "                    \n",
    "                else:\n",
    "                    left_X = X[X[:,best_col]<=best_split]\n",
    "                    left_y = y[X[:,best_col]<=best_split]\n",
    "                    right_X = X[X[:,best_col]>best_split]\n",
    "                    right_y = y[X[:,best_col]>best_split]\n",
    "                    left_subtree = self.grow_tree(left_X, left_y, types, max_depth, depth+1)\n",
    "                    right_subtree = self.grow_tree(right_X, right_y, types, max_depth, depth+1)\n",
    "                    node.children = [left_subtree, right_subtree]\n",
    "                return node\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        '''\n",
    "        Predicted classes for X\n",
    "        Args:\n",
    "            X: numpy array of data [num_samples, num_features]\n",
    "        Returns:\n",
    "            y: [num_samples, 1] predicted classes\n",
    "        '''\n",
    "        y_pred = []\n",
    "        for x in X:\n",
    "            y_pred.append(self.predict(x))\n",
    "        return y_pred\n",
    "    def predict(self, x):\n",
    "        node = self.root\n",
    "        while node.is_leaf == False:\n",
    "            node = node.get_children(x)\n",
    "        return node.value\n",
    "    def get_nodes(self):\n",
    "        if not self.root:\n",
    "            return []\n",
    "        bfs = deque([self.root])\n",
    "        nodes = []\n",
    "        while bfs:\n",
    "            curr = bfs.popleft()\n",
    "            nodes.append(curr)\n",
    "            if curr.children:\n",
    "                bfs.extend(curr.children)\n",
    "        return nodes\n",
    "    def post_prune(self, X_val, y_val):\n",
    "        nodes = self.get_nodes()\n",
    "        best_acc = 0\n",
    "        best_node = None\n",
    "        acc = accuracy_score(self(X_val),y_val)\n",
    "        while 1:\n",
    "            \n",
    "            for node in nodes:\n",
    "                if node.is_leaf:\n",
    "                    continue\n",
    "                children = node.children\n",
    "                node.children = []\n",
    "                node.is_leaf = True\n",
    "                node.value = Counter(y_val.flatten()).most_common(1)[0][0]\n",
    "                y_pred = self(X_val)\n",
    "                new_acc = accuracy_score(y_pred , y_val)\n",
    "                if best_acc <new_acc-acc:\n",
    "                    best_acc = new_acc-acc\n",
    "                    best_node = node\n",
    "                    # print(best_acc)\n",
    "                node.children = children\n",
    "                node.is_leaf = False\n",
    "            if best_acc>0:\n",
    "                best_node.is_leaf = True\n",
    "                best_node.children = []\n",
    "                acc = best_acc\n",
    "                nodes = self.get_nodes()\n",
    "            else:\n",
    "                break\n",
    "        print(\"Best Accuracy:\",acc)\n",
    "                \n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = None \n",
    "X_train,y_train = get_np_array('train.csv')\n",
    "label_encoder = None \n",
    "# X_train = X_train[:,:len(types)]\n",
    "X_test, y_test = get_np_array(\"test.csv\")\n",
    "print(np.mean(y_train==1))\n",
    "print(np.mean(y_test==1))\n",
    "# X_test = X_test[:,:len(types)]\n",
    "# print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "types = ['cat','cat','cat',\"cat\",\"cat\",\"cont\",\"cat\",\"cat\",\"cat\" ,\"cont\",\"cont\" ,\"cont\" ]\n",
    "while(len(types) != X_train.shape[1]):\n",
    "        types = ['cat'] + types\n",
    "for max_depth in [15,25,35,45]:\n",
    "        print(\"Max Depth: \",max_depth)\n",
    "        tree = DTTree()\n",
    "        tree.fit(X_train,y_train,types, max_depth = max_depth)\n",
    "        # tree.post_prune(X_val, y_val)\n",
    "        y_pred = tree(X_train)\n",
    "        print(\"Training Accuracy: \",accuracy_score(y_train, y_pred))\n",
    "        y_pred = tree(X_test)\n",
    "        print(\"Testing Accuracy: \",accuracy_score(y_test, y_pred))\n",
    "        y_pred = tree(X_val)\n",
    "        print(\"Validation Accuracy: \",accuracy_score(y_val, y_pred))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data\n",
    "depth = [15, 25, 35, 45]\n",
    "train_accuracy = [0.8243, 0.7892, 0.7443, 0.7089]\n",
    "test_accuracy = [0.5678, 0.6223, 0.6254, 0.6390]\n",
    "validation_accuracy = [0.6053, 0.6126, 0.6199, 0.6252]\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot lines for each accuracy type\n",
    "plt.plot(depth, train_accuracy, marker='o', label='Train Accuracy')\n",
    "plt.plot(depth, test_accuracy, marker='o', label='Test Accuracy')\n",
    "plt.plot(depth, validation_accuracy, marker='o', label='Validation Accuracy')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('Depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs. Depth')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = None \n",
    "X_train,y_train = get_np_array2('train.csv')\n",
    "label_encoder = None \n",
    "# X_train = X_train[:,:len(types)]\n",
    "X_test, y_test = get_np_array2(\"test.csv\")\n",
    "# X_test = X_test[:,:len(types)]\n",
    "# print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "types = ['cat','cat','cat',\"cat\",\"cat\",\"cont\",\"cat\",\"cat\",\"cat\" ,\"cont\",\"cont\" ,\"cont\" ]\n",
    "while(len(types) != X_train.shape[1]):\n",
    "        types = ['cat'] + types\n",
    "for max_depth in [15, 25, 35, 45]:\n",
    "        print(\"Max Depth: \",max_depth)\n",
    "        tree = DTTree()\n",
    "        tree.fit(X_train,y_train,types, max_depth = max_depth)\n",
    "        # print(tree.root)\n",
    "        y_pred = tree(X_train)\n",
    "        print(\"Training Accuracy: \",accuracy_score(y_train, y_pred))\n",
    "        y_pred = tree(X_test)\n",
    "        print(\"Testing Accuracy: \",accuracy_score(y_test, y_pred))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data for Table 1: Varying Depth\n",
    "depth = np.array([25, 35, 45, 15])\n",
    "train_accuracy = np.array([0.9881, 0.9996, 1.0000, 0.8243])\n",
    "test_accuracy = np.array([0.6246, 0.6184, 0.6070, 0.6070])\n",
    "val_accuracy = np.array([0.6126, 0.6299, 0.6322, 0.6448])\n",
    "\n",
    "# Data for Table 2: Varying CCP Values\n",
    "ccp_values = np.array([0.1, 0.01, 0.2, 0.001])\n",
    "train_accuracy_ccp = np.array([0.5034, 0.5344, 0.5034, 0.6946])\n",
    "test_accuracy_ccp = np.array([0.4964, 0.5181, 0.4964, 0.6319])\n",
    "val_accuracy_ccp = np.array([0.4736, 0.5, 0.4736, 0.6402])\n",
    "\n",
    "# Sort the data in ascending order\n",
    "depth_sorted = depth[np.argsort(depth)]\n",
    "train_accuracy_sorted = train_accuracy[np.argsort(depth)]\n",
    "test_accuracy_sorted = test_accuracy[np.argsort(depth)]\n",
    "val_accuracy_sorted = val_accuracy[np.argsort(depth)]\n",
    "\n",
    "ccp_values_sorted = ccp_values[np.argsort(ccp_values)]\n",
    "train_accuracy_ccp_sorted = train_accuracy_ccp[np.argsort(ccp_values)]\n",
    "test_accuracy_ccp_sorted = test_accuracy_ccp[np.argsort(ccp_values)]\n",
    "val_accuracy_ccp_sorted = val_accuracy_ccp[np.argsort(ccp_values)]\n",
    "\n",
    "# Plot Table 1: Different Accuracies vs Depth (Ascending)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(depth_sorted, train_accuracy_sorted, marker='o', label='Train Accuracy')\n",
    "plt.plot(depth_sorted, test_accuracy_sorted, marker='o', label='Test Accuracy')\n",
    "plt.plot(depth_sorted, val_accuracy_sorted, marker='o', label='Val Accuracy')\n",
    "plt.xlabel('Depth ')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracies vs Depth ')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot Table 2: Different Accuracies vs CCP Value (Ascending)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(ccp_values_sorted, train_accuracy_ccp_sorted, marker='o', label='Train Accuracy')\n",
    "plt.plot(ccp_values_sorted, test_accuracy_ccp_sorted, marker='o', label='Test Accuracy')\n",
    "plt.plot(ccp_values_sorted, val_accuracy_ccp_sorted, marker='o', label='Val Accuracy')\n",
    "plt.xlabel('CCP Value ')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracies vs CCP Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data for Part I: Without One-Hot Encoding\n",
    "depth_part1 = np.array([5, 10, 15, 20, 25])\n",
    "train_accuracy_part1 = np.array([0.5510, 0.5592, 0.5592, 0.5592, 0.5592])\n",
    "test_accuracy_part1 = np.array([0.5377, 0.5387, 0.5387, 0.5387, 0.5387])\n",
    "\n",
    "# Data for Part II: With One-Hot Encoding\n",
    "depth_part2 = np.array([15, 25, 35, 45])\n",
    "train_accuracy_part2 = np.array([0.7053, 0.8483, 0.9245, 0.9900])\n",
    "test_accuracy_part2 = np.array([0.5584, 0.6174, 0.6143, 0.6112])\n",
    "\n",
    "# Plot Part I: Depth vs Accuracies\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(depth_part1, train_accuracy_part1, marker='o', label='Training Accuracy ')\n",
    "plt.plot(depth_part1, test_accuracy_part1, marker='o', label='Test Accuracy ')\n",
    "plt.xlabel('Depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Depth vs Accuracies (Without One-Hot Encoding)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot Part II: Depth vs Accuracies\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(depth_part2, train_accuracy_part2, marker='o', label='Training Accuracy')\n",
    "plt.plot(depth_part2, test_accuracy_part2, marker='o', label='Test Accuracy ')\n",
    "plt.xlabel('Depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Depth vs Accuracies (With One-Hot Encoding)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "layer_size = [1,5,10,50,100]\n",
    "f1_average_train = [0.31,0.72,0.79,0.83,0.83]\n",
    "f1_average_test = [0.28,0.69,0.77,0.79,0.80]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(layer_size, f1_average_train, marker='o', label='Training F1 Score')\n",
    "plt.plot(layer_size, f1_average_test, marker='o', label='Test F1 Score')\n",
    "plt.xlabel('Layer Size')\n",
    "plt.xscale('log')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Score vs Layer Size')\n",
    "plt.xticks(layer_size, layer_size)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [[512],[512,256],[512,256,128],[512,256,128,64]]\n",
    "l_x = [1,2,3,4]\n",
    "f1_average_train = [0.83,0.79,1.00,1.00]\n",
    "f1_average_test = [0.81,0.78,0.82,0.83]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(l_x, f1_average_train, marker='o', label='Training F1 Score')\n",
    "plt.plot(l_x, f1_average_test, marker='o', label='Test F1 Score')\n",
    "plt.xlabel('Layers')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Score vs Layers (with Sigmoid)')\n",
    "plt.xticks(l_x, layers)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [[512],[512,256],[512,256,128],[512,256,128,64]]\n",
    "l_x = [1,2,3,4]\n",
    "f1_average_train = [0.69,0.85,0.87,0.89]\n",
    "f1_average_test = [0.69,0.80,0.83,0.82]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(l_x, f1_average_train, marker='o', label='Training F1 Score')\n",
    "plt.plot(l_x, f1_average_test, marker='o', label='Test F1 Score')\n",
    "plt.xlabel('Layers')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title(' Adaptive Learning')\n",
    "plt.xticks(l_x, layers)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [[512],[512,256],[512,256,128],[512,256,128,64]]\n",
    "l_x = [1,2,3,4]\n",
    "f1_average_train = [0.80,0.85,0.81,0.83]\n",
    "f1_average_test = [0.78,0.82,0.80,0.82]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(l_x, f1_average_train, marker='o', label='Training F1 Score')\n",
    "plt.plot(l_x, f1_average_test, marker='o', label='Test F1 Score')\n",
    "plt.xlabel('Layers')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Score vs Layers (with ReLu)')\n",
    "plt.xticks(l_x, layers)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [[512],[512,256],[512,256,128],[512,256,128,64]]\n",
    "l_x = [1,2,3,4]\n",
    "f1_average_train = [0.56,0.60,0.62,0.63]\n",
    "f1_average_test = [0.54,0.59,0.61,0.62]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(l_x, f1_average_train, marker='o', label='Training F1 Score')\n",
    "plt.plot(l_x, f1_average_test, marker='o', label='Test F1 Score')\n",
    "plt.xlabel('Layers')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('with MLP')\n",
    "plt.xticks(l_x, layers)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
