{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\veera\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, sgd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from PIL import Image\n",
    "import time\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_path = \"col_774_A4_2023/SyntheticData\"\n",
    "hw_path = \"col_774_A4_2023/HandwrittenData\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_bleu(true_vals,pred_vals):\n",
    "    ans=0\n",
    "    for i in range(len(true_vals)):\n",
    "        lst = len(true_vals[i].split(\" \"))\n",
    "        weight_lst = tuple((1/lst for _ in range(lst)))\n",
    "        ans+=nltk.translate.bleu_score.sentence_bleu([true_vals[i].split(\" \")],\n",
    "                                                     pred_vals[i].split(\" \"),\n",
    "                                                     weights=weight_lst,\n",
    "                                                     smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method4)\n",
    "    return ans/len(true_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_maker(path):\n",
    "    vocab = defaultdict(lambda : -1)\n",
    "    vocab[\"[PAD]\"] = 0\n",
    "    vocab[\"<SOS>\"] = 1\n",
    "    vocab[\"<EOS>\"] = 2\n",
    "    for file in path:\n",
    "        csv = pd.read_csv(file)\n",
    "        for formula in csv[\"formula\"]:\n",
    "            formula1 = formula.split(\" \")\n",
    "            for word in formula1:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = len(vocab)\n",
    "    return vocab\n",
    "@torch.no_grad()\n",
    "def load_data(path_to_img,path_to_csv,vocab,max_length=128):\n",
    "    imgs=[];formulas=[];formulas_lens=[]\n",
    "    transform = transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                # transforms.Normalize((0.5,), (0.5,)),\n",
    "                ])\n",
    "    mappings = pd.read_csv(path_to_csv)\n",
    "    formula_split = [mappings.iloc[i][\"formula\"].split(\" \") for i in range(len(mappings))]\n",
    "    with_max_length = [len(formula_split[i])<=max_length-2 for i in range(len(formula_split)) ]\n",
    "    mappings = mappings.loc[with_max_length]\n",
    "    formula_split = [formula_split[i] for i in range(len(formula_split)) if with_max_length[i]]\n",
    "    images = ([(transform(Image.open(os.path.join(path_to_img, fname)).resize((224, 224)))) for fname in mappings['image']])\n",
    "    formula_lens = np.array([len(formula) for formula in formula_split])\n",
    "    labels = np.zeros((len(formula_split),max_length))\n",
    "    for i in range(len(formula_split)):\n",
    "        labels[i][0] = vocab[\"<SOS>\"]\n",
    "        for j in range(len(formula_split[i])):\n",
    "            labels[i][j+1] = vocab[formula_split[i][j]]\n",
    "        labels[i][len(formula_split[i])+1] = vocab[\"<EOS>\"]\n",
    "    return images,labels,formula_lens,vocab\n",
    "\n",
    "class latex_dataset(Dataset):\n",
    "    def __init__(self,images,labels,lens,vocab,max_length=128):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.lens = lens\n",
    "        self.vocab = vocab\n",
    "        self.max_length = max_length\n",
    "        self.inv_vocab = {v:k for k,v in vocab.items()}\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    def __getitem__(self,idx):\n",
    "        if self.images[idx].shape[0] == 1:\n",
    "            self.images[idx] = torch.cat([self.images[idx]]*3,dim=0)\n",
    "        return self.images[idx],self.labels[idx],self.lens[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:80] data. DefaultCPUAllocator: not enough memory: you tried to allocate 602112 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      2\u001b[0m vocab_train\u001b[38;5;241m=\u001b[39m vocab_maker([synth_path\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/train.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m----> 3\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43msynth_path\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/images\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43msynth_path\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/train.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mvocab_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m train_data2 \u001b[38;5;241m=\u001b[39m latex_dataset(train_data[\u001b[38;5;241m0\u001b[39m],train_data[\u001b[38;5;241m1\u001b[39m],train_data[\u001b[38;5;241m2\u001b[39m],train_data[\u001b[38;5;241m3\u001b[39m])\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime taken to load train data: \u001b[39m\u001b[38;5;124m\"\u001b[39m,time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mstart)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(path_to_img, path_to_csv, vocab, max_length)\u001b[0m\n\u001b[0;32m     25\u001b[0m mappings \u001b[38;5;241m=\u001b[39m mappings\u001b[38;5;241m.\u001b[39mloc[with_max_length]\n\u001b[0;32m     26\u001b[0m formula_split \u001b[38;5;241m=\u001b[39m [formula_split[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(formula_split)) \u001b[38;5;28;01mif\u001b[39;00m with_max_length[i]]\n\u001b[1;32m---> 27\u001b[0m images \u001b[38;5;241m=\u001b[39m ([(transform(Image\u001b[38;5;241m.\u001b[39mopen(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path_to_img, fname))\u001b[38;5;241m.\u001b[39mresize((\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)))) \u001b[38;5;28;01mfor\u001b[39;00m fname \u001b[38;5;129;01min\u001b[39;00m mappings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[0;32m     28\u001b[0m formula_lens \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mlen\u001b[39m(formula) \u001b[38;5;28;01mfor\u001b[39;00m formula \u001b[38;5;129;01min\u001b[39;00m formula_split])\n\u001b[0;32m     29\u001b[0m labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(formula_split),max_length))\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     25\u001b[0m mappings \u001b[38;5;241m=\u001b[39m mappings\u001b[38;5;241m.\u001b[39mloc[with_max_length]\n\u001b[0;32m     26\u001b[0m formula_split \u001b[38;5;241m=\u001b[39m [formula_split[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(formula_split)) \u001b[38;5;28;01mif\u001b[39;00m with_max_length[i]]\n\u001b[1;32m---> 27\u001b[0m images \u001b[38;5;241m=\u001b[39m ([(\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_to_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m fname \u001b[38;5;129;01min\u001b[39;00m mappings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[0;32m     28\u001b[0m formula_lens \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mlen\u001b[39m(formula) \u001b[38;5;28;01mfor\u001b[39;00m formula \u001b[38;5;129;01min\u001b[39;00m formula_split])\n\u001b[0;32m     29\u001b[0m labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(formula_split),max_length))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\transforms\\functional.py:174\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    172\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[1;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_float_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:80] data. DefaultCPUAllocator: not enough memory: you tried to allocate 602112 bytes."
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "vocab_train= vocab_maker([synth_path+\"/train.csv\"])\n",
    "with \n",
    "train_data = load_data(synth_path+\"/images\",synth_path+\"/train.csv\",vocab_train)\n",
    "train_data2 = latex_dataset(train_data[0],train_data[1],train_data[2],train_data[3])\n",
    "print(\"Time taken to load train data: \",time.time()-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to load test data:  67.63750958442688\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "vocab_test_synth = vocab_maker([synth_path+\"/test.csv\"])\n",
    "test_data = load_data(synth_path+\"/images\",synth_path+\"/test.csv\",vocab_test_synth)\n",
    "test_data2 = latex_dataset(test_data[0],test_data[1],test_data[2],test_data[3])\n",
    "print(\"Time taken to load test data: \",time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to load val data:  83.11203646659851\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "vocab_val_synth = vocab_maker([synth_path+\"/val.csv\"])\n",
    "val_data = load_data(synth_path+\"/images\",synth_path+\"/val.csv\",vocab_val_synth)\n",
    "val_data2 = latex_dataset(val_data[0],val_data[1],val_data[2],val_data[3])\n",
    "print(\"Time taken to load val data: \",time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "train_data_HW = load_data(hw_path+\"/images/train\",hw_path+\"/train_hw.csv\",vocab_train)\n",
    "train_data_HW2 = latex_dataset(train_data_HW[0],train_data_HW[1],train_data_HW[2],train_data_HW[3])\n",
    "print(\"Time taken to load train data: \",time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = Dataset(train_data2,batch_size=32,shuffle=True)\n",
    "val_loader = Dataset(val_data2,batch_size=32,shuffle=True)\n",
    "test_loader = Dataset(test_data2,batch_size=32,shuffle=True)\n",
    "train_loader_hw = Dataset(train_data_HW2,batch_size=32,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.resnet = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features,512)\n",
    "    def forward(self,x):\n",
    "        return self.resnet(x)\n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self,encoder_dim,decoder_dim,attention_dim):\n",
    "        super(Attention,self).__init__()\n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.decoder_dim = decoder_dim\n",
    "        self.attention_dim = attention_dim\n",
    "        self.W = nn.Linear(decoder_dim,attention_dim)\n",
    "        self.U = nn.Linear(encoder_dim,attention_dim)\n",
    "        self.V = nn.Linear(attention_dim,1)\n",
    "    def forward(self,query,keys):\n",
    "        query = self.W(query)\n",
    "        keys = self.U(keys)\n",
    "        # keys = keys.unsqueeze(1)\n",
    "        attention = self.V(torch.tanh(query+keys)).squeeze(2).unsqueeze(1)\n",
    "        attention = F.softmax(attention,dim=1)\n",
    "        context = torch.bmm(attention,keys)\n",
    "        return context,attention\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,hidden_size, vocab_size, num_layers, vocabulary, max_seq_length=128):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_layers = num_layers\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.embedding = nn.Embedding(vocab_size,hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size,hidden_size,num_layers,batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.attention = Attention(hidden_size)\n",
    "        self.vocab  =  dict(vocabulary)\n",
    "\n",
    "    def forward(self,encoder_outputs, target_tensor=None, teacher_forcing_prob = 0.5):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(self.vocab[\"<SOS>\"])\n",
    "        decoder_hidden = (encoder_outputs.view(1,encoder_outputs.shape[0],encoder_outputs.shape[1]),encoder_outputs.view(1,encoder_outputs.shape[0],encoder_outputs.shape[1]))\n",
    "        decoder_outputs = []\n",
    "        attentions = []\n",
    "\n",
    "        for i in range(self.max_seq_length):\n",
    "            decoder_output, decoder_hidden, attention = self.forward_step(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            attentions.append(attention)\n",
    "            if target_tensor is not None and np.random.rand()<teacher_forcing_prob:\n",
    "                decoder_input = target_tensor[:,i].unsqueeze(1)\n",
    "            else:\n",
    "                _,topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(2).detach()\n",
    "                # decoder_input = decoder_output.argmax(dim=2)\n",
    "        decoder_outputs = torch.cat(decoder_outputs,dim=1)\n",
    "        attentions = torch.cat(attentions,dim=1)\n",
    "        return decoder_hidden,decoder_outputs,attentions\n",
    "    \n",
    "    def forward_step(self,input,hidden,encoder_output):\n",
    "        embedding = self.embedding(input)\n",
    "        query = hidden[0].permute(1,0,2)\n",
    "        context,attention = self.attention(query,encoder_output)\n",
    "        lstm_input = torch.cat([embedding,context],dim=-1)\n",
    "        lstm_input = torch.concat([encoder_output,lstm_input],dim=-1)\n",
    "        lstm_output, hidden = self.lstm(lstm_input,hidden)\n",
    "        output = self.out(lstm_output)\n",
    "\n",
    "        return output,hidden,attention\n",
    "    \n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,hidden_size, vocab_size, num_layers, vocabulary,max_seq_length=128):\n",
    "        super(Seq2Seq,self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder(hidden_size,vocab_size,num_layers,vocabulary,max_seq_length)\n",
    "    def forward(self,x, formula = None,teacher_forcing_prob=0.5):\n",
    "        encoder_output = self.encoder(x)\n",
    "        decoder_hidden,decoder_output,attentions = self.decoder(encoder_output,formula,teacher_forcing_prob)\n",
    "        return decoder_output,decoder_hidden,attentions\n",
    "    \n",
    "    def predict(self,x):\n",
    "        encoder_output = self.encoder(x)\n",
    "        decoder_hidden,decoder_output,attentions = self.decoder(encoder_output)\n",
    "        return decoder_output.argmax(-1),decoder_output\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq(1000,len(vocab_train),2,vocab_train)\n",
    "optimizer = Adam(model.parameters(),lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "model.to(device)\n",
    "criterion.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving models to LatexNet_ResNest\n"
     ]
    }
   ],
   "source": [
    "path = f'LatexNet_ResNest'\n",
    "os.mkdir(path)\n",
    "prev_loss = float('inf')\n",
    "stochastic_losses = []\n",
    "use_adaptive_tf = False\n",
    "print(f\"Saving models to {path}\")\n",
    "teacher_forcing_prob = 0.5\n",
    "latest_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1):\n",
    "    latest_epoch = epoch\n",
    "    model.train()\n",
    "    losses = []\n",
    "    times=[]\n",
    "    for i,(img,label,lens) in enumerate(train_loader):\n",
    "        img = img.to(device)\n",
    "        label = label.long().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output,_,_ = model(img,label,teacher_forcing_prob)\n",
    "        loss = criterion(output.reshape(-1,len(vocab_train)),label.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        stochastic_losses.append(loss.item())\n",
    "        if i%200==0:\n",
    "            print(f\"Epoch {epoch} Batch {i} Loss: {loss.item()}\")\n",
    "    print(f\"Epoch {epoch} Loss: {np.mean(losses)}\")\n",
    "    torch.save(model.state_dict(),f\"{path}/model_{epoch}.pth\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = Seq2Seq(1000,len(vocab_train),2,vocab_train)\n",
    "model_test.load_state_dict(torch.load(f\"{path}/model_{latest_epoch}.pth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vocab_inverse = {v:k for (k,v) in vocab_test_synth.items()}\n",
    "train_vocab_inverse = {v:k for (k,v) in vocab_train.items()}\n",
    "val_vocab_inverse = {v:k for (k,v) in vocab_val_synth.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test.eval()\n",
    "model_test.to(device)\n",
    "true_vals = []\n",
    "pred_vals = []\n",
    "images = []\n",
    "for data in test_loader:\n",
    "    decoder_outputs = model_test.forward(data[0].to(device),teacher_forcing_prob =1)[0].argmax(dim = -1)\n",
    "    for sent,true_sent,img in zip(decoder_outputs,data[1],data[0]):\n",
    "        s = []\n",
    "        images.append(img)\n",
    "        for i in sent[1:]:\n",
    "            if train_vocab_inverse[i.item()] == \"<EOS>\":\n",
    "                break\n",
    "            s.append(train_vocab_inverse[i.item()])\n",
    "        pred_vals.append(' '.join(s))\n",
    "        s = []\n",
    "        for i in true_sent:\n",
    "            if test_vocab_inverse[i.item()] == \"<EOS>\":\n",
    "                break\n",
    "            s.append(test_vocab_inverse[i.item()])\n",
    "        true_vals.append(' '.join(s[1:]))\n",
    "        \n",
    "\n",
    "print(\"Macro Bleu for Test Data set: \", macro_bleu(true_vals, pred_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_test\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
